{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flDp51OP7ygE",
        "outputId": "a4d5708c-8533-4103-cb40-38c79ce30e2f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8dkpznXx7ygL"
      },
      "outputs": [],
      "source": [
        "batch_size = 100  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 20000  # Number of samples to train on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Q7ZmlBRK7ygM"
      },
      "outputs": [],
      "source": [
        "\n",
        "column_names = ['ENGLISH', 'FRENCH']\n",
        "\n",
        "# Read the CSV file with named columns\n",
        "df = pd.read_csv('/content/fra.csv', header=None, names=column_names, encoding='latin-1')\n",
        "df_1=df.sample(frac=0.95, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW61CEcm7ygN",
        "outputId": "a1809a0f-443e-48b6-9cc3-b10f03c970b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-09c935406cb1>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df_1 = df_1.apply(lambda x: x.str.replace(r'\\W', ' ') if x.dtype == 'O' else x, axis=0)\n"
          ]
        }
      ],
      "source": [
        "#removing all special characters that are not letters or numbers in our text data ,replacing the removed characters with spaces and keeping spaces between words without removal to distinguish between words\n",
        "df_1 = df_1.apply(lambda x: x.str.replace(r'\\W', ' ') if x.dtype == 'O' else x, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YD1Qe7En7ygO"
      },
      "outputs": [],
      "source": [
        "df_1 = df_1.applymap(lambda x: x.lower() if isinstance(x, str) else x) # lower_case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Fzt9fT767ygO"
      },
      "outputs": [],
      "source": [
        "#creating function to apply tokenization\n",
        "def tokenize(column):\n",
        "    tokens = nltk.word_tokenize(column)\n",
        "    return [w for w in tokens if w.isalpha()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "B_pM3aXu7ygP"
      },
      "outputs": [],
      "source": [
        "df_1 = df_1.applymap(lambda x: tokenize(x) if isinstance(x, str) else x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IZLbBR7h7ygQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "input_column_name = 'ENGLISH'\n",
        "target_column_name = 'FRENCH'\n",
        "\n",
        "# Vectorize the data\n",
        "input_texts = df_1[input_column_name].astype(str).tolist()\n",
        "target_texts = ['\\t' + text + '\\n' for text in df_1[target_column_name].astype(str).tolist()]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "D2St9YKF7ygR"
      },
      "outputs": [],
      "source": [
        "# Build character sets\n",
        "input_characters = set(''.join(input_texts))\n",
        "target_characters = set(''.join(target_texts))\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])   #Finds the length of the longest input sequence among all the input_texts.\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8H33RepC7ygT",
        "outputId": "0fa02c03-39f7-4f33-a5e0-c4dfd8fe4e76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples: 44578\n",
            "Number of unique input tokens: 32\n",
            "Number of unique output tokens: 45\n",
            "Max sequence length for inputs: 47\n",
            "Max sequence length for outputs: 118\n"
          ]
        }
      ],
      "source": [
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7bHcI0qJ7ygU"
      },
      "outputs": [],
      "source": [
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])  #creates a list of tuples where each tuple contains a character and its corresponding index.\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "#encoding text sequences into numerical sequences by replacing characters with their respective indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SVC9qwMU7ygU"
      },
      "outputs": [],
      "source": [
        "# Text Preprocessing\n",
        "# One-hot encoding for encoder and decoder input data\n",
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
        "#These arrays serve as placeholders to store the encoded input and target sequences.\n",
        "# During the training process, the actual encoded sequences will be filled into these arrays to feed into the encoder and decoder of the sequence-to-sequence model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Y_yBw3097ygV"
      },
      "outputs": [],
      "source": [
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):          #For each character in the input_text \n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.  #it retrieves its index input_token_index dictionary \n",
        "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.   #sets the corresponding position in the encoder_input_data to 1.  'O-H'\n",
        "    for t, char in enumerate(target_text):\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.   ##it retrieves its index target_token_index dictionary \n",
        "        if t > 0:\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.     # sets the position in decoder_target_data to 1 for the previous character\n",
        "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.              # the model learns by predicting the next token given the previous one\n",
        "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RUurxWRc7ygV"
      },
      "outputs": [],
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jmktOrqo7ygW"
      },
      "outputs": [],
      "source": [
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "opHdlq3c7ygW"
      },
      "outputs": [],
      "source": [
        "#Train a basic LSTM-based Seq2Seq model to predict decoder_target_data given encoder_input_data and decoder_input_data\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDs-yqed7ygW",
        "outputId": "bff0974a-bdf1-40f0-e935-3d322fc6902b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "510/510 [==============================] - 18s 27ms/step - loss: 0.7442 - accuracy: 0.8050 - val_loss: 0.4543 - val_accuracy: 0.8695\n",
            "Epoch 2/100\n",
            "510/510 [==============================] - 11s 21ms/step - loss: 0.4359 - accuracy: 0.8731 - val_loss: 0.3968 - val_accuracy: 0.8811\n",
            "Epoch 3/100\n",
            "510/510 [==============================] - 11s 21ms/step - loss: 0.3757 - accuracy: 0.8869 - val_loss: 0.3637 - val_accuracy: 0.8907\n",
            "Epoch 4/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.3648 - accuracy: 0.8924 - val_loss: 0.3353 - val_accuracy: 0.8989\n",
            "Epoch 5/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.3274 - accuracy: 0.9005 - val_loss: 0.3218 - val_accuracy: 0.9025\n",
            "Epoch 6/100\n",
            "510/510 [==============================] - 12s 24ms/step - loss: 0.3102 - accuracy: 0.9056 - val_loss: 0.3061 - val_accuracy: 0.9062\n",
            "Epoch 7/100\n",
            "510/510 [==============================] - 11s 21ms/step - loss: 0.2960 - accuracy: 0.9096 - val_loss: 0.2948 - val_accuracy: 0.9105\n",
            "Epoch 8/100\n",
            "510/510 [==============================] - 12s 24ms/step - loss: 0.2827 - accuracy: 0.9133 - val_loss: 0.2830 - val_accuracy: 0.9128\n",
            "Epoch 9/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.2701 - accuracy: 0.9169 - val_loss: 0.2704 - val_accuracy: 0.9171\n",
            "Epoch 10/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.2599 - accuracy: 0.9200 - val_loss: 0.2629 - val_accuracy: 0.9187\n",
            "Epoch 11/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.2554 - accuracy: 0.9215 - val_loss: 0.2558 - val_accuracy: 0.9213\n",
            "Epoch 12/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.2498 - accuracy: 0.9231 - val_loss: 0.2505 - val_accuracy: 0.9226\n",
            "Epoch 13/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.2430 - accuracy: 0.9250 - val_loss: 0.2466 - val_accuracy: 0.9239\n",
            "Epoch 14/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.2387 - accuracy: 0.9263 - val_loss: 0.2474 - val_accuracy: 0.9244\n",
            "Epoch 15/100\n",
            "510/510 [==============================] - 12s 24ms/step - loss: 0.2340 - accuracy: 0.9274 - val_loss: 0.2360 - val_accuracy: 0.9267\n",
            "Epoch 16/100\n",
            "510/510 [==============================] - 12s 24ms/step - loss: 0.2260 - accuracy: 0.9300 - val_loss: 0.2271 - val_accuracy: 0.9298\n",
            "Epoch 17/100\n",
            "510/510 [==============================] - 12s 24ms/step - loss: 0.2207 - accuracy: 0.9316 - val_loss: 0.2244 - val_accuracy: 0.9308\n",
            "Epoch 18/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.2159 - accuracy: 0.9330 - val_loss: 0.2223 - val_accuracy: 0.9311\n",
            "Epoch 19/100\n",
            "510/510 [==============================] - 12s 25ms/step - loss: 0.2114 - accuracy: 0.9345 - val_loss: 0.2165 - val_accuracy: 0.9332\n",
            "Epoch 20/100\n",
            "510/510 [==============================] - 12s 24ms/step - loss: 0.2076 - accuracy: 0.9357 - val_loss: 0.2123 - val_accuracy: 0.9347\n",
            "Epoch 21/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.2039 - accuracy: 0.9369 - val_loss: 0.2110 - val_accuracy: 0.9351\n",
            "Epoch 22/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.2008 - accuracy: 0.9377 - val_loss: 0.2136 - val_accuracy: 0.9342\n",
            "Epoch 23/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1978 - accuracy: 0.9386 - val_loss: 0.2055 - val_accuracy: 0.9365\n",
            "Epoch 24/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1949 - accuracy: 0.9395 - val_loss: 0.2026 - val_accuracy: 0.9376\n",
            "Epoch 25/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1923 - accuracy: 0.9403 - val_loss: 0.2018 - val_accuracy: 0.9375\n",
            "Epoch 26/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1898 - accuracy: 0.9410 - val_loss: 0.2016 - val_accuracy: 0.9373\n",
            "Epoch 27/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1876 - accuracy: 0.9417 - val_loss: 0.1966 - val_accuracy: 0.9391\n",
            "Epoch 28/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1854 - accuracy: 0.9422 - val_loss: 0.1965 - val_accuracy: 0.9392\n",
            "Epoch 29/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1834 - accuracy: 0.9429 - val_loss: 0.1938 - val_accuracy: 0.9403\n",
            "Epoch 30/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1814 - accuracy: 0.9436 - val_loss: 0.1941 - val_accuracy: 0.9400\n",
            "Epoch 31/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1797 - accuracy: 0.9440 - val_loss: 0.1910 - val_accuracy: 0.9410\n",
            "Epoch 32/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1777 - accuracy: 0.9445 - val_loss: 0.1905 - val_accuracy: 0.9409\n",
            "Epoch 33/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1761 - accuracy: 0.9450 - val_loss: 0.1895 - val_accuracy: 0.9412\n",
            "Epoch 34/100\n",
            "510/510 [==============================] - 12s 23ms/step - loss: 0.1746 - accuracy: 0.9454 - val_loss: 0.1881 - val_accuracy: 0.9420\n",
            "Epoch 35/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1727 - accuracy: 0.9461 - val_loss: 0.1869 - val_accuracy: 0.9422\n",
            "Epoch 36/100\n",
            "510/510 [==============================] - 12s 23ms/step - loss: 0.1714 - accuracy: 0.9465 - val_loss: 0.1867 - val_accuracy: 0.9423\n",
            "Epoch 37/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1698 - accuracy: 0.9470 - val_loss: 0.1843 - val_accuracy: 0.9431\n",
            "Epoch 38/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1686 - accuracy: 0.9474 - val_loss: 0.1836 - val_accuracy: 0.9431\n",
            "Epoch 39/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1673 - accuracy: 0.9476 - val_loss: 0.1837 - val_accuracy: 0.9432\n",
            "Epoch 40/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1658 - accuracy: 0.9481 - val_loss: 0.1821 - val_accuracy: 0.9438\n",
            "Epoch 41/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1644 - accuracy: 0.9486 - val_loss: 0.1833 - val_accuracy: 0.9433\n",
            "Epoch 42/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1632 - accuracy: 0.9489 - val_loss: 0.1829 - val_accuracy: 0.9436\n",
            "Epoch 43/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1619 - accuracy: 0.9493 - val_loss: 0.1833 - val_accuracy: 0.9433\n",
            "Epoch 44/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1607 - accuracy: 0.9496 - val_loss: 0.1816 - val_accuracy: 0.9439\n",
            "Epoch 45/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1595 - accuracy: 0.9500 - val_loss: 0.1793 - val_accuracy: 0.9448\n",
            "Epoch 46/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1583 - accuracy: 0.9503 - val_loss: 0.1794 - val_accuracy: 0.9447\n",
            "Epoch 47/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1572 - accuracy: 0.9508 - val_loss: 0.1823 - val_accuracy: 0.9435\n",
            "Epoch 48/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1560 - accuracy: 0.9511 - val_loss: 0.1801 - val_accuracy: 0.9444\n",
            "Epoch 49/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1550 - accuracy: 0.9513 - val_loss: 0.1785 - val_accuracy: 0.9452\n",
            "Epoch 50/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1539 - accuracy: 0.9517 - val_loss: 0.1772 - val_accuracy: 0.9456\n",
            "Epoch 51/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1529 - accuracy: 0.9521 - val_loss: 0.1771 - val_accuracy: 0.9456\n",
            "Epoch 52/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1519 - accuracy: 0.9523 - val_loss: 0.1764 - val_accuracy: 0.9457\n",
            "Epoch 53/100\n",
            "510/510 [==============================] - 12s 23ms/step - loss: 0.1511 - accuracy: 0.9525 - val_loss: 0.1761 - val_accuracy: 0.9457\n",
            "Epoch 54/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1500 - accuracy: 0.9529 - val_loss: 0.1755 - val_accuracy: 0.9461\n",
            "Epoch 55/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1491 - accuracy: 0.9532 - val_loss: 0.1755 - val_accuracy: 0.9463\n",
            "Epoch 56/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1480 - accuracy: 0.9535 - val_loss: 0.1778 - val_accuracy: 0.9455\n",
            "Epoch 57/100\n",
            "510/510 [==============================] - 12s 23ms/step - loss: 0.1472 - accuracy: 0.9538 - val_loss: 0.1733 - val_accuracy: 0.9470\n",
            "Epoch 58/100\n",
            "510/510 [==============================] - 12s 23ms/step - loss: 0.1463 - accuracy: 0.9541 - val_loss: 0.1734 - val_accuracy: 0.9470\n",
            "Epoch 59/100\n",
            "510/510 [==============================] - 12s 23ms/step - loss: 0.1454 - accuracy: 0.9543 - val_loss: 0.1753 - val_accuracy: 0.9465\n",
            "Epoch 60/100\n",
            "510/510 [==============================] - 12s 23ms/step - loss: 0.1447 - accuracy: 0.9545 - val_loss: 0.1732 - val_accuracy: 0.9471\n",
            "Epoch 61/100\n",
            "510/510 [==============================] - 12s 23ms/step - loss: 0.1439 - accuracy: 0.9548 - val_loss: 0.1785 - val_accuracy: 0.9452\n",
            "Epoch 62/100\n",
            "510/510 [==============================] - 12s 23ms/step - loss: 0.1429 - accuracy: 0.9550 - val_loss: 0.1772 - val_accuracy: 0.9456\n",
            "Epoch 63/100\n",
            "510/510 [==============================] - 13s 26ms/step - loss: 0.1422 - accuracy: 0.9553 - val_loss: 0.1727 - val_accuracy: 0.9473\n",
            "Epoch 64/100\n",
            "510/510 [==============================] - 12s 23ms/step - loss: 0.1415 - accuracy: 0.9555 - val_loss: 0.1739 - val_accuracy: 0.9470\n",
            "Epoch 65/100\n",
            "510/510 [==============================] - 12s 23ms/step - loss: 0.1406 - accuracy: 0.9558 - val_loss: 0.1731 - val_accuracy: 0.9473\n",
            "Epoch 66/100\n",
            "510/510 [==============================] - 13s 26ms/step - loss: 0.1398 - accuracy: 0.9560 - val_loss: 0.1723 - val_accuracy: 0.9474\n",
            "Epoch 67/100\n",
            "510/510 [==============================] - 12s 23ms/step - loss: 0.1391 - accuracy: 0.9562 - val_loss: 0.1874 - val_accuracy: 0.9431\n",
            "Epoch 68/100\n",
            "510/510 [==============================] - 13s 26ms/step - loss: 0.1384 - accuracy: 0.9564 - val_loss: 0.1727 - val_accuracy: 0.9474\n",
            "Epoch 69/100\n",
            "510/510 [==============================] - 13s 26ms/step - loss: 0.1376 - accuracy: 0.9567 - val_loss: 0.1729 - val_accuracy: 0.9476\n",
            "Epoch 70/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1369 - accuracy: 0.9568 - val_loss: 0.1715 - val_accuracy: 0.9483\n",
            "Epoch 71/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1363 - accuracy: 0.9570 - val_loss: 0.1723 - val_accuracy: 0.9478\n",
            "Epoch 72/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1356 - accuracy: 0.9573 - val_loss: 0.1712 - val_accuracy: 0.9481\n",
            "Epoch 73/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1348 - accuracy: 0.9574 - val_loss: 0.1713 - val_accuracy: 0.9482\n",
            "Epoch 74/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1343 - accuracy: 0.9576 - val_loss: 0.1718 - val_accuracy: 0.9480\n",
            "Epoch 75/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1334 - accuracy: 0.9579 - val_loss: 0.1726 - val_accuracy: 0.9479\n",
            "Epoch 76/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1329 - accuracy: 0.9580 - val_loss: 0.1740 - val_accuracy: 0.9477\n",
            "Epoch 77/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1324 - accuracy: 0.9582 - val_loss: 0.1722 - val_accuracy: 0.9482\n",
            "Epoch 78/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1316 - accuracy: 0.9584 - val_loss: 0.1701 - val_accuracy: 0.9488\n",
            "Epoch 79/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1310 - accuracy: 0.9586 - val_loss: 0.1778 - val_accuracy: 0.9462\n",
            "Epoch 80/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1304 - accuracy: 0.9588 - val_loss: 0.1719 - val_accuracy: 0.9483\n",
            "Epoch 81/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1297 - accuracy: 0.9590 - val_loss: 0.1744 - val_accuracy: 0.9476\n",
            "Epoch 82/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1292 - accuracy: 0.9592 - val_loss: 0.1697 - val_accuracy: 0.9490\n",
            "Epoch 83/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1287 - accuracy: 0.9593 - val_loss: 0.1705 - val_accuracy: 0.9490\n",
            "Epoch 84/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1280 - accuracy: 0.9596 - val_loss: 0.1719 - val_accuracy: 0.9486\n",
            "Epoch 85/100\n",
            "510/510 [==============================] - 12s 23ms/step - loss: 0.1275 - accuracy: 0.9597 - val_loss: 0.1722 - val_accuracy: 0.9487\n",
            "Epoch 86/100\n",
            "510/510 [==============================] - 12s 23ms/step - loss: 0.1272 - accuracy: 0.9598 - val_loss: 0.1704 - val_accuracy: 0.9491\n",
            "Epoch 87/100\n",
            "510/510 [==============================] - 12s 23ms/step - loss: 0.1264 - accuracy: 0.9600 - val_loss: 0.1704 - val_accuracy: 0.9492\n",
            "Epoch 88/100\n",
            "510/510 [==============================] - 11s 23ms/step - loss: 0.1258 - accuracy: 0.9602 - val_loss: 0.1720 - val_accuracy: 0.9487\n",
            "Epoch 89/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1253 - accuracy: 0.9604 - val_loss: 0.1733 - val_accuracy: 0.9485\n",
            "Epoch 90/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1247 - accuracy: 0.9606 - val_loss: 0.1709 - val_accuracy: 0.9491\n",
            "Epoch 91/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1243 - accuracy: 0.9607 - val_loss: 0.1716 - val_accuracy: 0.9489\n",
            "Epoch 92/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1238 - accuracy: 0.9608 - val_loss: 0.1712 - val_accuracy: 0.9494\n",
            "Epoch 93/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1232 - accuracy: 0.9610 - val_loss: 0.1724 - val_accuracy: 0.9489\n",
            "Epoch 94/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1228 - accuracy: 0.9611 - val_loss: 0.1717 - val_accuracy: 0.9491\n",
            "Epoch 95/100\n",
            "510/510 [==============================] - 11s 22ms/step - loss: 0.1246 - accuracy: 0.9606 - val_loss: 0.1715 - val_accuracy: 0.9492\n",
            "Epoch 96/100\n",
            "510/510 [==============================] - 12s 23ms/step - loss: 0.1220 - accuracy: 0.9614 - val_loss: 0.1714 - val_accuracy: 0.9492\n",
            "Epoch 97/100\n",
            "510/510 [==============================] - 12s 23ms/step - loss: 0.1214 - accuracy: 0.9616 - val_loss: 0.1724 - val_accuracy: 0.9491\n",
            "Epoch 98/100\n",
            "510/510 [==============================] - 13s 25ms/step - loss: 0.1209 - accuracy: 0.9617 - val_loss: 0.1709 - val_accuracy: 0.9496\n",
            "Epoch 99/100\n",
            "510/510 [==============================] - 12s 23ms/step - loss: 0.1203 - accuracy: 0.9618 - val_loss: 0.1706 - val_accuracy: 0.9497\n",
            "Epoch 100/100\n",
            "510/510 [==============================] - 12s 23ms/step - loss: 0.1200 - accuracy: 0.9620 - val_loss: 0.1727 - val_accuracy: 0.9489\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)\n",
        "# Save model\n",
        "model.save('s2s.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ndqmDqpz7ygX"
      },
      "outputs": [],
      "source": [
        "# Next: inference mode (sampling).\n",
        "# Here's the drill:\n",
        "# 1) encode input and retrieve initial decoder state\n",
        "# 2) run one step of decoder with this initial state\n",
        "# and a \"start of sequence\" token as target.\n",
        "# Output will be the next target token\n",
        "# 3) Repeat with the current target token and current states\n",
        "\n",
        "# Define sampling models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wlfec3zx7ygX"
      },
      "outputs": [],
      "source": [
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ypjp12Vh7ygX"
      },
      "outputs": [],
      "source": [
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7sL3o6wI7ygX"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXDgq78Y7ygY",
        "outputId": "0acad936-a318-43c7-d0f3-13ee955b17fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "-\n",
            "Input sentence: ['how', 'are', 'you', 'getting', 'on']\n",
            "Decoded sentence: ['comment', 'pourrais', 'je', 'rester']\n",
            "\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "-\n",
            "Input sentence: ['everyone', 'was', 'fighting']\n",
            "Decoded sentence: ['tout', 'le', 'monde', 'est', 'en', 'train', 'de', 'dÃ©miser']\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "-\n",
            "Input sentence: ['that', 'could', 'be', 'wrong']\n",
            "Decoded sentence: ['cette', 'porte', 'est', 'propre']\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "-\n",
            "Input sentence: ['you', 'can', 't', 'handle', 'it']\n",
            "Decoded sentence: ['tu', 'ne', 'peux', 'pas', 'me', 'lever']\n",
            "\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "-\n",
            "Input sentence: ['i', 'let', 'them', 'go']\n",
            "Decoded sentence: ['j', 'aime', 'les', 'cheveux']\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "-\n",
            "Input sentence: ['do', 'you', 'want', 'something']\n",
            "Decoded sentence: ['aimez', 'vous', 'le', 'sac']\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "-\n",
            "Input sentence: ['what', 's', 'in', 'your', 'hand']\n",
            "Decoded sentence: ['qu', 'est', 'ce', 'que', 'tu', 'as', 'fait']\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "-\n",
            "Input sentence: ['look', 'who', 's', 'there']\n",
            "Decoded sentence: ['regarde', 'ce', 'qui', 'est', 'ici']\n",
            "\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        }
      ],
      "source": [
        "for seq_index in range(100):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcJb4fAX7ygY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        " \n",
        "# Splitting the sample data into training and testing sets \n",
        "train_df, test_df = train_test_split(df_1, test_size=0.2, random_state=42) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1XmRHkO7ygY"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu \n",
        "from rouge import Rouge \n",
        "from nltk.translate.meteor_score import meteor_score \n",
        " \n",
        "# Assuming you have separate datasets for training and evaluation \n",
        "train_input_texts = train_df[input_column_name].astype(str).tolist() \n",
        "train_target_texts = ['\\t' + text + '\\n' for text in train_df[target_column_name].astype(str).tolist()] \n",
        " \n",
        "eval_input_texts = test_df[input_column_name].astype(str).tolist() \n",
        "eval_target_texts = ['\\t' + text + '\\n' for text in test_df[target_column_name].astype(str).tolist()] \n",
        " \n",
        "# Function to clean and preprocess the input sequence \n",
        "def preprocess_input_sequence(input_seq): \n",
        "    # Implement any necessary preprocessing for the input sequence \n",
        "    return input_seq \n",
        " \n",
        "# Function to decode a sequence using the trained model \n",
        "def decode_sequence(input_seq): \n",
        "    \n",
        "    return decoded_sentence \n",
        " \n",
        "# Evaluate BLEU, ROUGE, and METEOR scores for training and evaluation datasets \n",
        "def evaluate_dataset(input_texts, target_texts): \n",
        "    bleu_scores = [] \n",
        "    rouge_scores = [] \n",
        "    meteor_scores = [] \n",
        " \n",
        "    for i in range(len(input_texts)): \n",
        "        input_seq = preprocess_input_sequence(input_texts[i]) \n",
        "        decoded_sentence = decode_sequence(input_seq) \n",
        " \n",
        "        # Reference sentences for the current sequence \n",
        "        reference = [tokenize(target_texts[i])] \n",
        " \n",
        "        # BLEU score \n",
        "        bleu_score = sentence_bleu(reference, tokenize(decoded_sentence)) \n",
        "        bleu_scores.append(bleu_score) \n",
        " \n",
        "        # ROUGE score \n",
        "        rouge = Rouge() \n",
        "        rouge_score = rouge.get_scores(decoded_sentence, target_texts[i]) \n",
        "        rouge_scores.append(rouge_score) \n",
        " \n",
        "        # METEOR score \n",
        "        meteor_score_value = meteor_score(target_texts[i], decoded_sentence) \n",
        "        meteor_scores.append(meteor_score_value) \n",
        " \n",
        "    return bleu_scores, rouge_scores, meteor_scores \n",
        " \n",
        "# Evaluate on training dataset \n",
        "train_bleu, train_rouge, train_meteor = evaluate_dataset(train_input_texts, train_target_texts) \n",
        " \n",
        "# Evaluate on evaluation dataset \n",
        "eval_bleu, eval_rouge, eval_meteor = evaluate_dataset(eval_input_texts, eval_target_texts) \n",
        " \n",
        "# Print or use the evaluation scores as needed \n",
        "print(f'Training BLEU Score: {sum(train_bleu) / len(train_bleu)}') \n",
        "print(f'Training ROUGE Scores: {sum(train_rouge) / len(train_rouge)}') \n",
        "print(f'Training METEOR Score: {sum(train_meteor) / len(train_meteor)}') \n",
        " \n",
        "print(f'Evaluation BLEU Score: {sum(eval_bleu) / len(eval_bleu)}') \n",
        "print(f'Evaluation ROUGE Scores: {sum(eval_rouge) / len(eval_rouge)}') \n",
        "print(f'Evaluation METEOR Score: {sum(eval_meteor) / len(eval_meteor)}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
